{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8466bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156c816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that detects the face, using the Dlib library methods.\n",
    "# In addition, let's combine some of the functionality with cv2, so we can render the rectangles on people's faces\n",
    "# that were detected.\n",
    "#Lastly, after detecing the face, crop out the said face and process it through the FER to determine an emotion\n",
    "def detectFaceDlib(detector, frame, data, inHeight=1650, inWidth=0):\n",
    "    frameDlibHog = frame.copy()\n",
    "    frameHeight = frameDlibHog.shape[0]\n",
    "    frameWidth = frameDlibHog.shape[1]\n",
    "    \n",
    "    if not inWidth:\n",
    "        inWidth = int((frameWidth / frameHeight) * inHeight)\n",
    "    \n",
    "    scaleHeight = frameHeight / inHeight\n",
    "    scaleWidth = frameWidth / inWidth\n",
    "    \n",
    "    frameDlibHogSmall = cv2.resize(frameDlibHog, (inWidth, inHeight))\n",
    "    faceRects = detector(frameDlibHogSmall, 0)\n",
    "    #print(frameWidth, frameHeight, inWidth, inHeight)\n",
    "    bboxes = []\n",
    "    \n",
    "    for faceRect in faceRects:\n",
    "        cvRect = [int(faceRect.left() * scaleWidth), int(faceRect.top() * scaleHeight), int(faceRect.right() * scaleWidth), int(faceRect.bottom() * scaleHeight)]\n",
    "        bboxes.append(cvRect)\n",
    "        cv2.rectangle(frameDlibHog, (cvRect[0], cvRect[1]), (cvRect[2], cvRect[3]), (0, 255, 0), int(round(frameHeight/250)), 4)    \n",
    "    \n",
    "    for i, d in enumerate(faceRects):\n",
    "        crop = frameDlibHogSmall[d.top():d.bottom(), d.left():d.right()]\n",
    "        result = DeepFace.analyze(crop, actions = ['emotion'], enforce_detection =  False)\n",
    "        if(result['dominant_emotion'] == 'neutral'):\n",
    "            data[0] += 1\n",
    "        elif(result['dominant_emotion'] == 'angry'):\n",
    "            data[1] += 1\n",
    "        elif(result['dominant_emotion'] == 'disgust'):\n",
    "            data[2] += 1\n",
    "        elif(result['dominant_emotion'] == 'fear'):\n",
    "            data[3] += 1\n",
    "        elif(result['dominant_emotion'] == 'happy'):\n",
    "            data[4] += 1\n",
    "        elif(result['dominant_emotion'] == 'sad'):\n",
    "            data[5] += 1\n",
    "        elif(result['dominant_emotion'] == 'surprise'):\n",
    "            data[6] += 1\n",
    "            \n",
    "    return frameDlibHog, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2cb1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want very high accuracy on facial recognition, keep the input height above 700.\n",
    "# However, the only downside of having high accuracy for facial recognition is low framerate.\n",
    "# If we want low accuracy but high framerate, then lower the input height below 500.\n",
    "# Expect the software to have issues when analyzing faces that are below 100 or so pixels.\n",
    "# It is typically accurate when the pixels are NOT below 100 pixels; otherwise, expect small frames to not be captured.\n",
    "\n",
    "# In addition, the individual needs to look at the camera for the software to recognize their face.\n",
    "# Otherwise, if they are NOT looking at the camera, their face will not be read.\n",
    "\n",
    "# Take aways\n",
    "# 1. higher facial detection accuracy = slower run-time.\n",
    "# 2. lower facial detection accuracy = faster run-time.\n",
    "# 3. If the lighting is bad, the face will not be read.\n",
    "# 4. If the frame is simply too small (less than 100 pixels), then expect the face to not be read.\n",
    "# 5. HOG outperforms MMOD in every way, including accuracy. It also doesn't need a .dat library.\n",
    "# 6. 1200 for input height seems to be the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5996ed5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 26\u001b[0m outDlibHog, bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetectFaceDlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhogFaceDetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m tt_dlibHog \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;241m-\u001b[39m t\n\u001b[0;32m     29\u001b[0m fpsDlibHog \u001b[38;5;241m=\u001b[39m frame_count \u001b[38;5;241m/\u001b[39m tt_dlibHog\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mdetectFaceDlib\u001b[1;34m(detector, frame, data, inHeight, inWidth)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(faceRects):\n\u001b[0;32m     27\u001b[0m     crop \u001b[38;5;241m=\u001b[39m frameDlibHogSmall[d\u001b[38;5;241m.\u001b[39mtop():d\u001b[38;5;241m.\u001b[39mbottom(), d\u001b[38;5;241m.\u001b[39mleft():d\u001b[38;5;241m.\u001b[39mright()]\n\u001b[1;32m---> 28\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdominant_emotion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     30\u001b[0m         data[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\deepface\\DeepFace.py:391\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, models, enforce_detection, detector_backend, prog_bar)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    390\u001b[0m \temotion_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisgust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurprise\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 391\u001b[0m \timg, region \u001b[38;5;241m=\u001b[39m \u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrayscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_region\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \temotion_predictions \u001b[38;5;241m=\u001b[39m models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(img)[\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m    395\u001b[0m \tsum_of_predictions \u001b[38;5;241m=\u001b[39m emotion_predictions\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\deepface\\commons\\functions.py:192\u001b[0m, in \u001b[0;36mpreprocess_face\u001b[1;34m(img, target_size, grayscale, enforce_detection, detector_backend, return_region, align)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m#--------------------------\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m#post-processing\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grayscale \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m \timg \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m#---------------------------------------------------\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m#resize image to expected shape\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# img = cv2.resize(img, target_size) #resize causes transformation on base image, adding black pixels to resize will not deform the base image\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = [0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "    \n",
    "    source = '00004.mp4' # File path a video here, it can be only accept one file as it should.\n",
    "    \n",
    "    cap = cv2.VideoCapture(source)\n",
    "    hasFrame, frame = cap.read()\n",
    "    \n",
    "    vid_writer = cv2.VideoWriter('output-hog-{}.avi'.format(str(source).split(\".\")[0]), cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 15, (frame.shape[1], frame.shape[0])) #Output the results onto a video.\n",
    "    \n",
    "    frame_count = 0 # We need a frame counter.\n",
    "    tt_dlibHog = 0\n",
    "    \n",
    "    while(1):\n",
    "        hasFrame, frame = cap.read()\n",
    "        \n",
    "        if not hasFrame:\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "        t = time.time()\n",
    "        \n",
    "        try:\n",
    "            outDlibHog, bboxes = detectFaceDlib(hogFaceDetector, frame, data\n",
    "                \n",
    "            tt_dlibHog += time.time()  - t\n",
    "            fpsDlibHog = frame_count / tt_dlibHog\n",
    "\n",
    "            label = \"DLIB HoG ; FPS : {:.2f}\".format(fpsDlibHog)\n",
    "            cv2.putText(outDlibHog, label, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"Face Detection Comparison\", outDlibHog)\n",
    "\n",
    "            vid_writer.write(outDlibHog)\n",
    "            if frame_count == 1:\n",
    "                tt_dlibHog = 0\n",
    "        except:\n",
    "            print(\"error\")\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    emotions = ('neutral', 'angry', 'disgust', 'fear', 'happy', 'sad', 'surprise')\n",
    "    y_pos = np.arange(len(emotions))\n",
    "    plt.bar(y_pos, data, align ='center', alpha = .5)\n",
    "    plt.xticks(y_pos, emotions)\n",
    "    plt.ylabel('Occurances')\n",
    "    plt.title('Number of Emotion Occurances in *video name*')\n",
    "    plt.show()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    vid_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02044333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7c882a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f31cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
